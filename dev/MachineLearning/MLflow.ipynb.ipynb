{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c3e95bb-3887-4bc9-b3f0-12ca4e1a3aec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#ML model Hyperparameter Tuning\n",
    "## Loading the CSV dataset in the DBFS (Databricks File System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a45356-1bc5-4741-a759-97c32f64ebf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %sh\n",
    " rm -r /dbfs/mlflow_lab\n",
    " mkdir /dbfs/mlflow_lab\n",
    " wget -O /dbfs/mlflow_lab/diabetes.csv https://raw.githubusercontent.com/fanidam91/smartdata-lab/main/dev/Data_AI/Data_ai/diabetes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "565e45c9-332e-46eb-b6a5-7d39466cc70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Splitting the dataset into train and test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045bc2d2-96e4-43b5-b68f-5ce6dd0bdd8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/mlflow_lab/diabetes.csv\")\n",
    "data = data.dropna().select(col(\"Pregnancies\").astype(\"int\"),\n",
    "                           col(\"Glucose\").astype(\"int\"),\n",
    "                          col(\"BloodPressure\").astype(\"int\"),\n",
    "                          col(\"SkinThickness\").astype(\"int\"),\n",
    "                          col(\"Insulin\").astype(\"int\"),\n",
    "                          col(\"BMI\").astype(\"float\"),\n",
    "                          col(\"DiabetesPedigreeFunction\").astype(\"float\"),\n",
    "                          col(\"Age\").astype(\"int\"),\n",
    "                          col(\"Outcome\").astype(\"int\")\n",
    "                          )\n",
    "\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dce2d63-e13c-4851-a977-61b43042be38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Optimizing Hyperparameter values for our ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bf0b05-50d8-45ba-8f53-5a8e8c8d6893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "import mlflow\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "   \n",
    "def objective(params):\n",
    "    # Train a model using the provided hyperparameter value\n",
    "    numFeatures = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "    numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "    numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "    featureVector = VectorAssembler(inputCols=[\"normalizedFeatures\"], outputCol=\"Features\")\n",
    "    mlAlgo = DecisionTreeClassifier(labelCol=\"Outcome\",    \n",
    "                                    featuresCol=\"Features\",\n",
    "                                    maxDepth=params['MaxDepth'], maxBins=params['MaxBins'])\n",
    "    pipeline = Pipeline(stages=[numVector, numScaler, featureVector, mlAlgo])\n",
    "    model = pipeline.fit(train)\n",
    "       \n",
    "    # Evaluate the model to get the target metric\n",
    "    prediction = model.transform(test)\n",
    "    eval = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = eval.evaluate(prediction)\n",
    "       \n",
    "    # Hyperopt tries to minimize the objective function, so you must return the negative accuracy.\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "     #\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15b22757-a8d9-4d95-bc67-8cd7a4d472c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Defining the Search Space for our hyperparameters\n",
    "##Also will be logging each hyperparameter run using a Trials() object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9aebbc6-0eb6-4a2a-9729-783b38381123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials\n",
    "from hyperopt import fmin, tpe, hp\n",
    "  \n",
    "search_space = {\n",
    "    'MaxDepth': hp.randint('MaxDepth', 10),\n",
    "    'MaxBins': hp.choice('MaxBins', [10, 20, 30])\n",
    "}\n",
    "\n",
    "algo=tpe.suggest\n",
    "\n",
    "# Create a Trials object to track each run\n",
    "trial_runs = Trials()\n",
    "   \n",
    "argmin = fmin(\n",
    "  fn=objective,\n",
    "  space=search_space,\n",
    "  algo=algo,\n",
    "  max_evals=3,\n",
    "  trials=trial_runs)\n",
    "   \n",
    "print(\"Best param values: \", argmin)\n",
    "   \n",
    "# Get details from each trial run\n",
    "print (\"trials:\")\n",
    "for trial in trial_runs.trials:\n",
    "    print (\"\\n\", trial)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5019957284001479,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MLflow.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
