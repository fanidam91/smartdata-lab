{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7017a960-3762-448f-a160-2294d2c5210b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üé¨ Iron Man (2008) ‚Äì J.A.R.V.I.S. as Deep Learning AI\n",
    "üß† Scene Analogy:\n",
    "When Tony Stark builds the Iron Man suit in his lab, J.A.R.V.I.S. doesn‚Äôt just follow commands ‚Äî he learns, adapts, and responds like a deep learning system.\n",
    "\n",
    "üîç Mapping to Deep Learning Concepts:\n",
    "Movie Element\tDeep Learning Equivalent\n",
    "J.A.R.V.I.S.\tAI model (multi-modal deep learning assistant)\n",
    "Tony feeding designs, voice inputs\tTraining data (supervised learning from input-output examples)\n",
    "Real-time feedback (e.g., flight test failures)\tReinforcement learning (trial and error)\n",
    "Speech interaction\tNatural Language Processing (NLP)\n",
    "Gesture and visual sensing\tComputer Vision + Sensor Fusion\n",
    "Self-upgrades (e.g., flight stabilizers)\tModel optimization and fine-tuning\n",
    "\n",
    "üéØ Key Scene to Refer:\n",
    "üî• \"Test Flight Scene\" ‚Äî Tony crashes multiple times while testing the suit.\n",
    "‚Üí Each failure is a feedback loop, like how reinforcement learning helps models learn optimal behavior.\n",
    "\n",
    "üí° Final Analogy:\n",
    "J.A.R.V.I.S. is like a Deep Learning assistant that learns Tony's preferences, builds the suit, corrects mistakes, and even holds conversations ‚Äî just like how LLMs + deep models adapt and evolve with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d107642-562d-483d-bbdb-dd990a2cacbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Building a Deep Learning Model\n",
    "##Loading the dataset into the DBFS (Databricks File System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6caa4dec-1d51-41a1-be51-e64b2db1959d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "rm -r /dbfs/deepml_lab\n",
    "mkdir /dbfs/deepml_lab\n",
    "wget -O /dbfs/deepml_lab/diabetes.csv https://raw.githubusercontent.com/fanidam91/smartdata-lab/main/dev/Data_AI/Data_ai/diabetes.csv\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d66addf5-5411-4fea-a2d7-e73460383c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c8d0cb-72cf-495c-912c-83cba355979b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  # <--- ADD THIS\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data, removing any incomplete rows\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/deepml_lab/diabetes.csv\").dropna()\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "for col_name in ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
    "                 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(\"float\"))\n",
    "\n",
    "# Split the data into training and testing datasets   \n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', \n",
    "            'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "label = 'Outcome'\n",
    "\n",
    "# Convert to Pandas and split\n",
    "pandas_df = df.toPandas()\n",
    "x_train, x_test, y_train, y_test = train_test_split(pandas_df[features].values,\n",
    "                                                    pandas_df[label].values,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Ensure correct data types\n",
    "x_train = x_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "print('Training Set: %d rows, Test Set: %d rows\\n' % (len(x_train), len(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5d20bd-aa1c-4426-8b10-f4480e801cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Install and import the PyTorch Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990bfb5e-f555-4dd5-bf5d-6aa5d2f7a9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as td\n",
    "import torch.nn.functional as F\n",
    "   \n",
    "# Set random seed for reproducability\n",
    "torch.manual_seed(0)\n",
    "   \n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796665ec-ac2a-43a6-85f3-57ba72a32baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f714ee20-53f2-4361-9577-09844aab131e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a dataset and loader for the training data and labels\n",
    "train_x = torch.Tensor(x_train).float()\n",
    "train_y = torch.Tensor(y_train).long()\n",
    "train_ds = td.TensorDataset(train_x,train_y)\n",
    "train_loader = td.DataLoader(train_ds, batch_size=20,\n",
    "    shuffle=False, num_workers=1)\n",
    "\n",
    "# Create a dataset and loader for the test data and labels\n",
    "test_x = torch.Tensor(x_test).float()\n",
    "test_y = torch.Tensor(y_test).long()\n",
    "test_ds = td.TensorDataset(test_x,test_y)\n",
    "test_loader = td.DataLoader(test_ds, batch_size=20,\n",
    "                             shuffle=False, num_workers=1)\n",
    "print('Ready to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27feba7c-987d-438c-bcdf-00f33a5cd376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc1a4ef-5e10-4aa1-98ed-90265cc74a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "h1 = 10 \n",
    "\n",
    "# Define the neural network\n",
    "class DiabetesNet(nn.Module):\n",
    " def __init__(self):\n",
    "    super(DiabetesNet, self).__init__()\n",
    "    self.fc1 = nn.Linear(len(features), h1) # defining the input layer\n",
    "    self.fc2 = nn.Linear(h1,h1) # defining the hidden layers\n",
    "    self.fc3 = nn.Linear(h1,2) # defining the output layer\n",
    "\n",
    " def forward(self, x):\n",
    "    fc1_output = torch.relu(self.fc1(x))\n",
    "    fc2_output = torch.relu(self.fc2(fc1_output))\n",
    "    y = F.log_softmax(self.fc3(fc2_output).float(), dim=1)\n",
    "    return y\n",
    "\n",
    "# Create a model instance from the network\n",
    "model = DiabetesNet()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365b8af8-a17f-452f-af7b-6b1e63ec81f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create Functions to Test and Train a Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c77c7e-2914-4061-b0da-11f69706f01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "\n",
    "    for batch,tensor in enumerate(data_loader):\n",
    "        data, target = tensor\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_criteria(out, target)\n",
    "        train_loss = train_loss + loss.item()\n",
    "\n",
    "        # backpropagate adjustments to the weight\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Return the average loss \n",
    "    avg_loss = train_loss / (batch+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "\n",
    "def test(model, data_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    # switch the model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "         batch_count = 0\n",
    "         for batch, tensor in enumerate(data_loader):\n",
    "             batch_count += 1\n",
    "             data, target = tensor\n",
    "             # get the predictions\n",
    "             out = model(data)\n",
    "\n",
    "             # calculate the loss\n",
    "             test_loss = loss_criteria(out, target).item() + test_loss\n",
    "\n",
    "             # calculate the accuracy\n",
    "             _, predicted = torch.max(out.data,1)\n",
    "             correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss/batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "       \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba48fe45-ab61-4615-95c7-80f2c5dd6eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d77fb6e-b9c2-4c2e-a999-325347d1b9c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specify the loss criteria (we'll use CrossEntropyLoss for multi-class classification)\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "   \n",
    "# Use an optimizer to adjust weights and reduce loss\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer.zero_grad()\n",
    "   \n",
    "# We'll track metrics for each epoch in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "   \n",
    "# Train over 100 epochs\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "   \n",
    "    # print the epoch number\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "       \n",
    "    # Feed training data into the model\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "       \n",
    "    # Feed the test data into the model to check its performance\n",
    "    test_loss = test(model, test_loader)\n",
    "       \n",
    "    # Log the metrics for this epoch\n",
    "    epoch_nums.append(epoch)\n",
    "    training_loss.append(train_loss)\n",
    "    validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cd97c2-9f0e-4989-84a3-d21d918d9cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Review the Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f4c0c8-378e-42ce-88fe-ff672cb4c422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "   \n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119de5db-3e9b-4e89-bd87-dfa014112796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View the learned weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a488763-6460-4141-9a24-c1bca29bc2b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\n\", model.state_dict()[param_tensor].numpy())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f1a6a8-680f-44d7-a319-28121dd4a1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save the trained model# Save the model weights\n",
    "model_file = '/dbfs/diabetes_predictor.pt'\n",
    "torch.save(model.state_dict(), model_file)\n",
    "del model\n",
    "print('model saved as', model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e451caa6-8c23-41a6-8030-cebc658acfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "model_file = '/dbfs/diabetes_predictor.pt'\n",
    "torch.save(model.state_dict(), model_file)\n",
    "del model\n",
    "print('model saved as', model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bab5d1b-0fd4-4188-a9d7-7ff534a106ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference/Test the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81071d22-1df5-405b-978c-55c19108d594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# New Diabetes Features\n",
    "x_new = [[8,85,65,29,0,26.6,0.672,32]]\n",
    "print ('New sample: {}'.format(x_new))\n",
    "   \n",
    "# Create a new model class and load weights\n",
    "model = DiabetesNet()\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "   \n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "   \n",
    "# Get a prediction for the new data sample\n",
    "x = torch.Tensor(x_new).float()\n",
    "_, predicted = torch.max(model(x).data, 1)\n",
    "   \n",
    "print('Prediction:',predicted.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3210bcfc-ca03-48d4-8c25-5aaaa71201a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Logging the Model using MLflow for real-time inferencing via an exposed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a46dc4b-a9d2-4cc3-be36-9073b3822a70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Log the model in MLflow\n",
    "with mlflow.start_run():   \n",
    "    mlflow.pytorch.log_model(model, \"diabetes_predictor_model\")\n",
    "    print(\"Model logged in MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c0de609-c104-47e4-920f-bd8866bc01f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Inferencing Model through the serving endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d27372-47e8-427d-a09f-59c1d97d731d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    " {\n",
    "   \"dataframe_records\": [\n",
    "   {\n",
    "      \"Pregnancies\": 8,\n",
    "      \"Glucose\": 85,\n",
    "      \"BloodPressure\": 65,\n",
    "      \"SkinThickness\": 29,\n",
    "      \"Insulin\": 0,\n",
    "      \"BMI\": 26.6,\n",
    "      \"DiabetesPedigreeFunction\": 0.672,\n",
    "      \"Age\": 34\n",
    "   }\n",
    "   ]\n",
    " }"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5019957284001483,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DeepLearning.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
