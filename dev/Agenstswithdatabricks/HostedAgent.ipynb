{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4222e37f-6ef6-4b2f-b2ab-eca52c420b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U mlflow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4070a343-ba1e-48da-bba1-4e914fb8eea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üîß Install and Initialize LangChain, LangGraph, and MLflow for Databricks Agent Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83548bee-f64b-4b8f-9c71-fdf1c12ae07d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U mlflow\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46773aa6-ec3e-4b86-a17a-ada68c7376a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.tools.databricks import UCFunctionToolkit\n",
    "import requests\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Use ModelConfig if config.yml is loaded earlier\n",
    "from mlflow.models import ModelConfig\n",
    "config = ModelConfig(development_config=\"config.yml\")\n",
    "\n",
    "# Create the LLM using correct config key\n",
    "llm = ChatDatabricks(endpoint=config.get(\"llm_endpoint\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd918b5-ddec-4a2b-8ed0-f7bda0dc0945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.langchain.autolog()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067244bc-ff1f-49d2-9e71-398499c10029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install -U -qqqq mlflow-skinny langchain==0.2.16 langgraph-checkpoint==1.0.12 langchain_core langchain-community==0.2.16 langgraph==0.2.16 pydantic databricks_langchain\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5867fc76-3949-4cca-966a-507f5211cf3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üîÅ Restart Python Kernel to Finalize Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c797382c-5f0f-4da0-861d-a2af7f157ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d34b17b-6969-4fda-8b53-0dbdaf08290c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üì¶ Enable MLflow Autologging and Load Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb864dea-1375-4b98-a8f7-23dd59544003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import ModelConfig\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "config = ModelConfig(development_config=\"config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ecd5e87-0c71-4894-96c8-801c270bf826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üõ†Ô∏è Initialize Databricks LLM, UC Tools, and Custom Weather Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d151f43-7033-431e-aced-5466203d5254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.tools.databricks import UCFunctionToolkit\n",
    "import requests\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Create the LLM using the correct key\n",
    "llm = ChatDatabricks(endpoint=config.get(\"llm_endpoint\"))\n",
    "\n",
    "# Load UC Functions\n",
    "uc_functions = config.get(\"uc_functions\") if config.get(\"uc_functions\") else []\n",
    "\n",
    "tools = (\n",
    "    UCFunctionToolkit(warehouse_id=config.get(\"warehouse_id\"))\n",
    "    .include(*uc_functions)\n",
    "    .get_tools()\n",
    ")\n",
    "\n",
    "# Define a custom weather tool\n",
    "def get_weather(location):\n",
    "    url = f\"http://api.openweathermap.org/geo/1.0/direct?q={location}&limit=1&appid=YOUR_API_KEY\"\n",
    "    response = requests.get(url).json()\n",
    "    lat = response[0]['lat']\n",
    "    lon = response[0]['lon']\n",
    "    \n",
    "    weather_url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid=YOUR_API_KEY\"\n",
    "    weather_response = requests.get(weather_url).json()\n",
    "    return weather_response['weather'][0]['description']\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    func=lambda location: get_weather(location),\n",
    "    description=\"Fetches weather information for a specified location. The location should be provided as a string (e.g., 'Mumbai').\"\n",
    ")\n",
    "\n",
    "tools.append(weather_tool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4744c488-f866-4895-9916-a0e8bb7c8960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üå¶Ô∏è Define and Register a Custom Weather Tool Using OpenWeatherMap API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0ec734-4264-448e-b567-e6a0a83eb99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_weather(location):\n",
    "            \"\"\"\n",
    "            Fetch the weather information for a particular location that the user is interested in.\n",
    "\n",
    "            Parameters:\n",
    "            location (string): the location whose weather information needs to be fetched\n",
    "\n",
    "            Returns:\n",
    "                weather (string): The weather information for the given location\n",
    "            \"\"\"\n",
    "            url = \"http://api.openweathermap.org/geo/1.0/direct?q=\" + location + \"&limit=1&appid=39f360dc1a168305bdbb5203ac7717b5\"\n",
    "            response=requests.get(url)\n",
    "            get_response=response.json()\n",
    "            latitude=get_response[0]['lat']\n",
    "            longitude = get_response[0]['lon']\n",
    "\n",
    "            url_final = \"https://api.openweathermap.org/data/2.5/weather?lat=\" + str(latitude) + \"&lon=\" + str(longitude) + \"&appid=39f360dc1a168305bdbb5203ac7717b5\"\n",
    "            final_response = requests.get(url_final)\n",
    "            final_response_json = final_response.json()\n",
    "            weather=final_response_json['weather'][0]['description']\n",
    "            print(weather)\n",
    "            return(weather)\n",
    "        \n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    func=lambda location: get_weather(location),\n",
    "    description=\"Fetches weather information for a specified location. The location should be provided as a string (e.g., 'Mumbai').\"\n",
    ")\n",
    "\n",
    "tools.append(weather_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26fde05f-3cc6-463a-877b-4a1878de0134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üß† Utility Functions for Formatting and Parsing LangChain Agent Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a4189b-8af2-4f75-a9f6-018f73de9c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict, Any\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    "    MessageLikeRepresentation,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def stringify_tool_call(tool_call: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert a raw tool call into a formatted string that the playground UI expects if there is enough information in the tool_call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        request = json.dumps(\n",
    "            {\n",
    "                \"id\": tool_call.get(\"id\"),\n",
    "                \"name\": tool_call.get(\"name\"),\n",
    "                \"arguments\": json.dumps(tool_call.get(\"args\", {})),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "        return f\"{request}\"\n",
    "    except:\n",
    "        return str(tool_call)\n",
    "\n",
    "\n",
    "def stringify_tool_result(tool_msg: ToolMessage) -> str:\n",
    "    \"\"\"\n",
    "    Convert a ToolMessage into a formatted string that the playground UI expects if there is enough information in the ToolMessage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = json.dumps(\n",
    "            {\"id\": tool_msg.tool_call_id, \"content\": tool_msg.content}, indent=2\n",
    "        )\n",
    "        return f\"{result}\"\n",
    "    except:\n",
    "        return str(tool_msg)\n",
    "\n",
    "\n",
    "def parse_message(msg) -> str:\n",
    "    \"\"\"Parse different message types into their string representations\"\"\"\n",
    "    # tool call result\n",
    "    if isinstance(msg, ToolMessage):\n",
    "        return stringify_tool_result(msg)\n",
    "    # tool call\n",
    "    elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "        tool_call_results = [stringify_tool_call(call) for call in msg.tool_calls]\n",
    "        return \"\".join(tool_call_results)\n",
    "    # normal HumanMessage or AIMessage (reasoning or final answer)\n",
    "    elif isinstance(msg, (AIMessage, HumanMessage)):\n",
    "        return msg.content\n",
    "    else:\n",
    "        print(f\"Unexpected message type: {type(msg)}\")\n",
    "        return str(msg)\n",
    "\n",
    "\n",
    "def wrap_output(stream: Iterator[MessageLikeRepresentation]) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Process and yield formatted outputs from the message stream.\n",
    "    The invoke and stream langchain functions produce different output formats.\n",
    "    This function handles both cases.\n",
    "    \"\"\"\n",
    "    for event in stream:\n",
    "        # the agent was called with invoke()\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                yield parse_message(msg) + \"\\n\\n\"\n",
    "        # the agent was called with stream()\n",
    "        else:\n",
    "            for node in event:\n",
    "                for key, messages in event[node].items():\n",
    "                    if isinstance(messages, list):\n",
    "                        for msg in messages:\n",
    "                            yield parse_message(msg) + \"\\n\\n\"\n",
    "                    else:\n",
    "                        print(\"Unexpected value {messages} for key {key}. Expected a list of `MessageLikeRepresentation`'s\")\n",
    "                        yield str(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031d8a5b-fa56-42d8-8350-b932d396eed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üîÅ Custom LangGraph Tool-Calling Agent Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1e7550-fbca-452e-8a10-71e0979b2e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "\n",
    "# We create the AgentState that we will pass around\n",
    "# This simply involves a list of messages\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolExecutor, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there is no function call, then we finish\n",
    "        if not last_message.tool_calls:\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = SystemMessage(content=agent_prompt)\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    # Define the function that calls the model\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        # First, we define the start node. We use agent.\n",
    "        # This means these are the edges taken after the agent node is called.\n",
    "        \"agent\",\n",
    "        # Next, we pass in the function that will determine which node is called next.\n",
    "        should_continue,\n",
    "        # The mapping below will be used to determine which node to go to\n",
    "        {\n",
    "            # If tools, then we call the tool node.\n",
    "            \"continue\": \"tools\",\n",
    "            # END is a special node marking that the graph should finish.\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    # We now add a unconditional edge from tools to agent.\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39d70fa-4686-4b0d-8f16-63fa9433d9aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üß† Build and Visualize a LangGraph Agent with Optional System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2fd4044-04a9-4021-a2b4-e42fb9d6ecca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from mlflow.langchain.output_parsers import ChatCompletionsOutputParser\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Create the agent with the system message if it exists\n",
    "try:\n",
    "    agent_prompt = config.get(\"agent_prompt\")\n",
    "    agent_with_raw_output = create_tool_calling_agent(\n",
    "        llm, tools, agent_prompt=agent_prompt\n",
    "    )\n",
    "    display(Image(agent_with_raw_output.get_graph().draw_mermaid_png()))\n",
    "\n",
    "except KeyError:\n",
    "    agent_with_raw_output = create_tool_calling_agent(llm, tools)\n",
    "agent = agent_with_raw_output | RunnableGenerator(wrap_output) | ChatCompletionsOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "349fbb1a-5975-4e17-817c-c6a2d6380009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Stream and Display Agent Response to a Sample Weather Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb52d6e0-dfa8-4a8c-b510-553946949de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: replace this placeholder input example with an appropriate domain-specific example for your agent\n",
    "for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"how much does green webcam cost?\"}]}):\n",
    "    print(event, \"---\" * 20 + \"\\n\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656b110f-f11c-4f6b-9baf-a4834fc7155a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üì¶ Log the LangChain Agent to MLflow for Tracking and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b642d0-5cb5-4662-9400-7c51f9ae89e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlflow.models.set_model(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ccc2012-9b24-4314-a76b-3ccc15008819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from mlflow.langchain.output_parsers import ChatCompletionsOutputParser\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def build_agent_pipeline(llm, tools, config, wrap_output):\n",
    "    \"\"\"\n",
    "    Constructs a LangChain agent pipeline that supports tool calling and output parsing.\n",
    "    \n",
    "    Args:\n",
    "        llm: A language model (e.g., OpenAI, AzureOpenAI).\n",
    "        tools: List of LangChain-compatible tools.\n",
    "        config: A dictionary that may include 'agent_prompt'.\n",
    "        wrap_output: A function to format/stream output.\n",
    "\n",
    "    Returns:\n",
    "        A composed agent pipeline ready for streaming.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        agent_prompt = config.get(\"agent_prompt\")\n",
    "        agent_with_raw_output = create_tool_calling_agent(\n",
    "            llm, tools, agent_prompt=agent_prompt\n",
    "        )\n",
    "        display(Image(agent_with_raw_output.get_graph().draw_mermaid_png()))\n",
    "    except KeyError:\n",
    "        agent_with_raw_output = create_tool_calling_agent(llm, tools)\n",
    "\n",
    "    # Build final runnable agent pipeline\n",
    "    agent = agent_with_raw_output | RunnableGenerator(wrap_output) | ChatCompletionsOutputParser()\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61dc87c1-c706-4336-abd8-cc582ee1b851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Input field\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Ask about a product...',\n",
    "    description='Query:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Output display\n",
    "output = widgets.Output()\n",
    "\n",
    "# Button\n",
    "run_button = widgets.Button(\n",
    "    description='Run Agent',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "# Callback\n",
    "def run_agent(b):\n",
    "    output.clear_output()\n",
    "    user_query = query_input.value\n",
    "\n",
    "    if not user_query:\n",
    "        with output:\n",
    "            print(\"Please enter a query.\")\n",
    "        return\n",
    "\n",
    "    with output:\n",
    "        print(f\"üß† Agent running on: '{user_query}'\")\n",
    "        for event in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": user_query}]}):\n",
    "            print(event, end=\"\")\n",
    "\n",
    "# Link button to callback\n",
    "run_button.on_click(run_agent)\n",
    "\n",
    "# Display widgets\n",
    "display(query_input, run_button, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68c5d2f-79b3-440e-b39b-635cba8d545b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üîß Spark Version:\", spark.version)\n",
    "print(\"üß† Cluster Node Type:\", spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\"))\n",
    "print(\"üß† Driver Node Type:\", spark.conf.get(\"spark.databricks.clusterUsageTags.driverNodeType\"))\n",
    "print(\"üî¢ Number of Executors:\", spark.conf.get(\"spark.databricks.clusterUsageTags.numExecutors\"))\n",
    "print(\"üïí Auto Termination (mins):\", spark.conf.get(\"spark.databricks.cluster.profile.autoTerminationMinutes\"))\n",
    "print(\"üì¶ Cluster ID:\", spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\"))\n",
    "print(\"üìç Region:\", spark.conf.get(\"spark.databricks.workspaceUrl\").split('.')[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45b71d7-2ba0-4e81-a083-2d227976fefc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"üß† Cluster Node Type:\", spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\", \"Not available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fccb1e-2ac3-4261-8c52-73c6a1cde906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for k, v in spark.conf.getAll.items():\n",
    "    if \"databricks\" in k:\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HostedAgent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
